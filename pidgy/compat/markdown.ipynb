{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `markdown_it and pidgy` compatability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from pidgy import util\n",
    "    import textwrap, markdown_it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Markdown` class maintains the operational logic to transform partially coded [Markdown] into fully coded Python. The translation happens in two steps:\n",
    "1. `Markdown.parse`/lex/tokenize the input string to tokens identifying Markdown blocks.\n",
    "2. `Markdown.render` the tokens into the target language format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class Markdown(markdown_it.MarkdownIt):\n",
    "        def parse(self, src, env=None, normalize=False):\n",
    "            src = enforce_blanklines(src)\n",
    "            if env is None:\n",
    "                env = markdown_it.utils.AttrDict()\n",
    "            env.update(src=src.splitlines(True))\n",
    "            tokens = super().parse(src, env)\n",
    "            if normalize: tokens = reconfigure_tokens(filter_tangle_tokens(tokens), env)\n",
    "            return tokens\n",
    "        def render(self, src, env=None):                \n",
    "            if env is None:\n",
    "                env  = markdown_it.utils.AttrDict()\n",
    "            return super().render(src, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class Renderer:        \n",
    "        __output__ = \"html\"\n",
    "        __init__ = markdown_it.renderer.RendererHTML.__init__\n",
    "\n",
    "        def render(self, tokens, options, env):\n",
    "            return \"\".join(env[\"src\"])\n",
    "\n",
    "        def quote(self, str, trailing=''):\n",
    "            \"\"\"Wrap a truple block quotations.\"\"\"\n",
    "            quote, length = self.QUOTES[self.QUOTES[0] in str], len(str)\n",
    "            left, right = length - len(str.lstrip()), len(str.rstrip())\n",
    "            if not str[left:right].strip(): return str\n",
    "            if str[right-1] == '\\\\':\n",
    "                while str[right-1] == '\\\\':\n",
    "                    right -= 1\n",
    "            else:\n",
    "                if str[left:right].endswith(quote[0]):\n",
    "                    quote = {\"'''\": '\"\"\"', '\"\"\"': \"'''\"}[quote]\n",
    "            return str[:left] + quote + str[left:right] + quote + trailing + str[right:]\n",
    "\n",
    "        def measure_base_indent(self, tokens, env):\n",
    "            next = self.get_next_code_token(tokens, -1)\n",
    "            if next and next.type == 'code_block':\n",
    "                env['base_indent'] = lead_indent(env['src'][slice(*next.map)])\n",
    "            else:\n",
    "                env['base_indent'] = 4\n",
    "                \n",
    "        def get_next_code_token(self, tokens, idx):\n",
    "            for token in tokens[idx+1:]:\n",
    "                if token.type in {'code_block'}:\n",
    "                    return token\n",
    "        \n",
    "        def hanging_indent(self, str, env):\n",
    "            start = len(str)-len(str.lstrip())\n",
    "            return str[:start] + ' '* env['extra_indent'] + str[start:]\n",
    "        \n",
    "        def indent(self, str, env):\n",
    "            return textwrap.indent(str, ' ' *env['base_indent'])\n",
    "\n",
    "\n",
    "        def token_to_str(self, tokens, idx, env):\n",
    "            if idx < len(tokens):\n",
    "                if tokens[idx] and tokens[idx].map:\n",
    "                    return ''.join(env['src'][slice(*tokens[idx].map)])\n",
    "            return \"\"\n",
    "        \n",
    "        def update_env(self, code, tokens, idx, env):\n",
    "            next = self.get_next_code_token(tokens, idx)\n",
    "            extra_indent = 0\n",
    "            if next:\n",
    "                extra_indent = max(0, lead_indent(env['src'][slice(*next.map)]) -env['base_indent'])\n",
    "            if not extra_indent and code.rstrip().endswith(\":\"):\n",
    "                extra_indent += 4\n",
    "            rstrip = code.rstrip()\n",
    "            env.update(\n",
    "                extra_indent=extra_indent,\n",
    "                base_indent=util.trailing_indent(code),\n",
    "                continued=rstrip.endswith('\\\\'), \n",
    "                quoted=rstrip.rstrip('\\\\').endswith(self.QUOTES)\n",
    "            )\n",
    "        def render(self, tokens, options, env):\n",
    "            env.update(base_indent=0, quoted=False, extra_indent=0, continued=False)\n",
    "            tokens = reconfigure_tokens(filter_tangle_tokens(tokens), env)\n",
    "            self.measure_base_indent(tokens, env)\n",
    "            if not tokens:\n",
    "                return self.quote(''.join(env['src']), trailing=';')\n",
    "            return textwrap.dedent(continuation(\n",
    "                markdown_it.renderer.RendererHTML.render(self, tokens, options, env), env\n",
    "            ) + \"\\n\" + self.noncode(tokens, len(tokens), env)).rstrip() + '\\n'        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utility functions for parsing and rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import doctest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    CODE_TYPES = \"fence code_block front_matter bullet_list_open ordered_list_open footnote_reference_open reference\".split()\n",
    "\n",
    "    def lead_indent(str):\n",
    "        \"\"\"Count the lead indent of a string\"\"\"\n",
    "        if not isinstance(str, list):\n",
    "            str = str.splitlines(True)\n",
    "        for line in str:\n",
    "            if line.strip():\n",
    "                return len(line) - len(line.lstrip())\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "    def filter_tangle_tokens(token, code=None):\n",
    "        \"\"\"Filter out tokens that reference a potential coded object.\"\"\"\n",
    "        code = code or []\n",
    "        if isinstance(token, list):\n",
    "            for token in token:\n",
    "                code = filter_tangle_tokens(token, code)\n",
    "        elif token.children:\n",
    "            for token in token.children:\n",
    "                code = filter_tangle_tokens(token, code)\n",
    "        else:\n",
    "            if token.type in CODE_TYPES:\n",
    "                if token.type == \"code_block\":\n",
    "                    while token.content.lstrip().startswith(\">>>\"):\n",
    "                        start, end = next(doctest.DocTestParser._EXAMPLE_RE.finditer(token.content)).span()\n",
    "                        token.map[0] = len(token.content[:end].splitlines())\n",
    "                        token.content = token.content[end:]\n",
    "                    if not token.content.strip(): return code\n",
    "                if token not in code:\n",
    "                    code.append(token)\n",
    "            if code and (code[-1].type == \"fence\") and code[-1].info:\n",
    "                code.pop(-1)\n",
    "        return code or [\n",
    "            markdown_it.utils.AttrDict(type=\"code_block\", content=\"\", map=(0, 0))\n",
    "        ]\n",
    "\n",
    "\n",
    "    def make_reference_tokens(env, *tokens):\n",
    "        \"\"\"Turn references in the markdown_it environment to tokens.\"\"\"\n",
    "        for reference in env.get(\"references\", {}).values():\n",
    "            if not tokens:\n",
    "                tokens += (markdown_it.token.Token(\"reference\", \"\", 1),)\n",
    "                tokens[-1].map = reference[\"map\"]\n",
    "                continue\n",
    "            for line in env[\"src\"][tokens[-1].map[1] : reference[\"map\"][0]]:\n",
    "                if line.strip():\n",
    "                    tokens += (markdown_it.token.Token(\"reference\", \"\", 1),)\n",
    "                    tokens[-1].map = reference[\"map\"]\n",
    "                    break\n",
    "            else:\n",
    "                tokens[-1].map[1] = reference[\"map\"][1]\n",
    "\n",
    "            tokens[-1].content = \"\".join(env[\"src\"][slice(*tokens[-1].map)])\n",
    "\n",
    "        return [recontent(x, env) for x in tokens if int.__sub__(*x.map)]\n",
    "\n",
    "\n",
    "    def recontent(token, env):\n",
    "        \"\"\"Update the content on a call.\"\"\"\n",
    "        token.content = \"\".join(env[\"src\"][slice(*token.map)])\n",
    "        return token\n",
    "\n",
    "\n",
    "    def reconfigure_tokens(tokens, env):\n",
    "        \"\"\"Tokens are miss ordered, this function splits and orders cells.\"\"\"\n",
    "        tokens = sorted(tokens + make_reference_tokens(env), key=lambda x: x.map[0])\n",
    "        new = tokens and [tokens[0]] or []\n",
    "        for token in tokens[1:]:\n",
    "            if token.map[0] < new[-1].map[1]:\n",
    "                new.extend([token, __import__('copy').deepcopy(new[-1])])\n",
    "                new[-3].map[1], new[-1].map[0] = token.map\n",
    "\n",
    "                for i in [-3, -1]:\n",
    "                    (\n",
    "                        new.pop(i)\n",
    "                        if int.__sub__(*new[i].map) == 0\n",
    "                        else recontent(new[i], env)\n",
    "                    )\n",
    "                continue\n",
    "            new.append(token)\n",
    "\n",
    "        return [x for x in new if int.__sub__(*x.map)]\n",
    "\n",
    "\n",
    "    def continuation(str, env):\n",
    "        \"\"\"Extend a line ending with a continuation.\"\"\"\n",
    "        lines, continuing = str.splitlines(), False\n",
    "        for i, line in enumerate(lines):\n",
    "            if line.strip():\n",
    "                continuing = line.endswith(\"\\\\\")\n",
    "            elif continuing:\n",
    "                lines[i] = \" \" * env[\"base_indent\"] + \"\\\\\"\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def enforce_blanklines(str):\n",
    "        \"\"\"Make sure blank lines are blank.\"\"\"\n",
    "        str = \"\".join(\n",
    "            line if line.strip() else \"\\n\" for line in \"\".join(str).splitlines(True)\n",
    "        )\n",
    "        if not str.endswith(\"\\n\"):\n",
    "            str += \"\\n\"\n",
    "        return str\n",
    "\n",
    "    def quote_docstrings(str):\n",
    "        next, end = \"\", 0\n",
    "        for m in doctest.DocTestParser._EXAMPLE_RE.finditer(str):\n",
    "            next += str[slice(end, m.start())] + quote(\n",
    "                str[slice(m.start(), m.end())], trailing=\";\"\n",
    "            )\n",
    "            end = m.end()\n",
    "        if next:\n",
    "            next += str[m.end() :]\n",
    "        return next or str\n",
    "\n",
    "\n",
    "    def unfence(str):\n",
    "        \"\"\"Remove code fences froma string.\"\"\"\n",
    "        return \"\".join(\"\".join(str.split(\"```\", 1)).rsplit(\"```\", 1))\n",
    "\n",
    "\n",
    "    def dedent_block(str):\n",
    "        \"\"\"Dedent a block of non code.\"\"\"\n",
    "        str = textwrap.dedent(str)\n",
    "        lines = str.splitlines(True)\n",
    "        for i, line in enumerate(lines):\n",
    "            if line.strip():\n",
    "                lines[i] = textwrap.dedent(line)\n",
    "                break\n",
    "        return \"\".join(lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
