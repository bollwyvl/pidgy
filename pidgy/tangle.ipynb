{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translating [Markdown] to [Python]\n",
    "\n",
    "A primary translation is literate programming is the tangle step that converts the literate program into \n",
    "the programming language. The original implementation converts `\".WEB\"` files to valid pascal - `\".PAS\"` - files.\n",
    "The `pidgy` approach begins with [Markdown] files and proper [Python] files as the outcome. The rest of this \n",
    "document configures how [IPython] acknowledges the transformation and the heuristics the translate [Markdown] to [Python].\n",
    "\n",
    "[Markdown]: #\n",
    "[Python]: #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import typing, mistune, IPython, pidgy.util, ast, textwrap, markdown_it\n",
    "    __all__ = 'tangle', 'Tangle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pidgy` tangle workflow has three steps:\n",
    "\n",
    "1. Block-level lexical analysis to tokenize [Markdown].\n",
    "2. Normalize the tokens to compacted `\"code\" and not \"code\"` tokens.\n",
    "3. Translate the normalized tokens to a string of valid [Python] code.\n",
    "\n",
    "[Markdown]: #\n",
    "[Python]: #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class PythonRender(pidgy.util.BaseRenderer):\n",
    "        QUOTES = '\"\"\"', \"'''\"\n",
    "        def quote(self, str):\n",
    "            \"\"\"Wrap a truple block quotations.\"\"\"\n",
    "            quote, length = self.QUOTES[self.QUOTES[0] in str], len(str)\n",
    "            left, right = length - len(str.lstrip()), len(str.rstrip())\n",
    "            if left+right == length: return str\n",
    "            while str[right-1] == '\\\\':\n",
    "                right -= 1\n",
    "            return str[:left] + quote + str[left:right] + quote + str[right:]\n",
    "\n",
    "        def measure_base_indent(self, tokens, env): \n",
    "            next = self.get_next_code_token(tokens, -1)\n",
    "            if next.type == 'code_block':\n",
    "                env['base_indent'] = pidgy.util.lead_indent(env['src'][slice(*next.map)])\n",
    "                \n",
    "        def get_next_code_token(self, tokens, idx):\n",
    "            for token in tokens[idx+1:]:\n",
    "                if token.type in {'code_block'}:\n",
    "                    return token\n",
    "        \n",
    "        def hanging_indent(self, str, env):\n",
    "            start = len(str)-len(str.lstrip())\n",
    "            return str[:start] + ' '* env['extra_indent'] + str[start:]\n",
    "        \n",
    "        def indent(self, str, env):\n",
    "            return textwrap.indent(str, ' ' *env['base_indent'])\n",
    "\n",
    "        def prepare_noncode(self, token, tokens, idx, env):\n",
    "            if token is None:\n",
    "                range, prior = slice(None), slice(*tokens[-1].map)\n",
    "            else:\n",
    "                range, prior = slice(*token.map), slice(*tokens[idx-1].map) if idx else slice(0,0)\n",
    "             \n",
    "            non_code = pidgy.util.dedent_block(''.join(env['src'][\n",
    "                prior.stop:range.start\n",
    "            ]))\n",
    "            non_code = self.indent(self.hanging_indent(non_code, env), env)\n",
    "            non_code = self.quote(non_code)\n",
    "            return non_code\n",
    "        \n",
    "        def prepare_front_matter(self, code, token, env):\n",
    "            if token.markup == '+++':\n",
    "                code = F'''locals().update(__import__('toml').loads(\"\"\"{code}\"\"\"))'''\n",
    "            elif token.markup == '---':\n",
    "                code = F'''locals().update(__import__('yaml').safe_load(\"\"\"{code}\"\"\"))'''            \n",
    "            return self.indent(code, env)\n",
    "        \n",
    "        def prepare_fence(self, code, env):\n",
    "            return pidgy.util.unfence(self.indent(code, env))\n",
    "        \n",
    "        def prepare_ref(self, code, env):\n",
    "            code = self.indent(self.hanging_indent(code, env), env)\n",
    "            return self.quote(code)\n",
    "        \n",
    "        def prepare_code(self, token, tokens, idx, env):\n",
    "            code = ''.join(env['src'][slice(*token.map)])\n",
    "            if token.type in {'fence', 'code_block'}:\n",
    "                if token.type == 'fence':\n",
    "                    code = textwrap.indent(code, ' '*env['base_token'])\n",
    "                env['base_indent'] = pidgy.util.trailing_indent(code)\n",
    "                env['quoted'] = code.rstrip().rstrip('\\\\').endswith(self.QUOTES)\n",
    "                code = pidgy.util.quote_docstrings(code)\n",
    "                \n",
    "            if token.type == 'front_matter':\n",
    "                code = self.prepare_front_matter(code, token, env)\n",
    "            \n",
    "            elif token.type == 'fence':\n",
    "                code = self.prepare_fence(code, env)\n",
    "                \n",
    "            elif token.type != 'code_block':\n",
    "                code = self.prepare_ref(code, env)\n",
    "                \n",
    "            return code\n",
    "\n",
    "        \n",
    "        def update_env(self, code, tokens, idx, env):\n",
    "            next = self.get_next_code_token(tokens, idx)\n",
    "            extra_indent = 0\n",
    "            if next:\n",
    "                extra_indent = max(0, pidgy.util.lead_indent(env['src'][slice(*next.map)]) -env['base_indent'])\n",
    "            if not extra_indent and code.rstrip().endswith(\":\"):\n",
    "                extra_indent += 4\n",
    "            env['extra_indent'] = extra_indent\n",
    "            \n",
    "\n",
    "        def renderToken(self, tokens, idx, options, env):\n",
    "            if not env['base_indent']: self.measure_base_indent(tokens, env)\n",
    "            token = tokens[idx]\n",
    "            non_code = self.prepare_noncode(token, tokens, idx, env)\n",
    "            code = self.prepare_code(token, tokens, idx, env)\n",
    "            self.update_env(code, tokens, idx, env)\n",
    "            string = non_code + code\n",
    "            \n",
    "            if len(tokens)-1 == idx:\n",
    "                string += self.prepare_noncode(None, tokens, idx, env)\n",
    "                \n",
    "            return string\n",
    "        \n",
    "\n",
    "        def render(self, tokens, options, env):\n",
    "            env.update(base_indent=0, quoted=False, extra_indent=0)\n",
    "            tokens = pidgy.util.reconfigure_tokens(pidgy.util.filter_tangle_tokens(tokens), env)\n",
    "            if not tokens:\n",
    "                return self.quote(''.join(env['src']),)\n",
    "            return textwrap.dedent(pidgy.util.continuation(markdown_it.renderer.RendererHTML.render(self, tokens, options, env),env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if __name__ != '__main__':\n",
    "        @pidgy.implementation\n",
    "        def tangle(str:str)->str:\n",
    "            translate = Tangle(renderer_cls=PythonRender)\n",
    "            return translate.stringify(''.join(str or []))\n",
    "    else:\n",
    "\n",
    "        @pidgy.implementation\n",
    "        def tangle(str:str)->str:\n",
    "            translate = Tangle()\n",
    "            return translate.stringify(translate.parse(''.join(str or [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class pidgyManager(IPython.core.inputtransformer2.TransformerManager):\n",
    "        def transform_cell(self, cell): return super(type(self), self).transform_cell(tangle(str=cell))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block level lexical analysis.\n",
    "\n",
    "`pidgy` uses a modified `mistune.BlockLexer` to create block level tokens\n",
    "for a [Markdown] source. A specific `pidgy` addition is the addition off \n",
    "a `doctest` block object, `doctest` are testable strings that are ignored by the tangle\n",
    "step. The tokens are to be normalized and translated to [Python] strings.\n",
    "\n",
    "<details><summary><code>BlockLexer</code></summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class BlockLexer(mistune.BlockLexer, pidgy.util.ContextDepth):\n",
    "        class grammar_class(mistune.BlockGrammar):\n",
    "            doctest = __import__('doctest').DocTestParser._EXAMPLE_RE\n",
    "            block_code = __import__('re').compile(r'^((?!\\s+>>>\\s) {4}[^\\n]+\\n*)+')\n",
    "            default_rules = \"newline hrule block_code fences heading nptable lheading block_quote list_block def_links def_footnotes table paragraph text\".split()\n",
    "\n",
    "        def parse_doctest(self, m): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
    "\n",
    "        def parse_fences(self, m):\n",
    "            if m.group(2): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
    "            else: super().parse_fences(m)\n",
    "\n",
    "        def parse_hrule(self, m): self.tokens.append(dict(type='hrule', text=m.group(0)))\n",
    "            \n",
    "        def parse_def_links(self, m):\n",
    "            super().parse_def_links(m)\n",
    "            self.tokens.append(dict(type='def_link', text=m.group(0)))\n",
    "            \n",
    "        def _resolve_front_matter(self, text): \n",
    "            lstrip = text.lstrip()\n",
    "            for lang, sep in zip(\"yaml toml\".split(), \"--- +++\".split()):\n",
    "                if lstrip and lstrip.startswith(F'{sep}\\n') and F'\\n{sep}\\n' in lstrip[len(sep):]:\n",
    "                    front_matter, sep, text = lstrip[len(sep):].partition(F'\\n{sep}\\n')\n",
    "                    return text, {'type': 'front_matter', 'text': F\"\\n{front_matter}\", \"lang\": lang}\n",
    "            return text, None\n",
    "\n",
    "        def parse(self, text: str, default_rules=None, normalize=True) -> typing.List[dict]:\n",
    "            if not self.depth: \n",
    "                self.tokens = []\n",
    "                text, front_matter = self._resolve_front_matter(text)\n",
    "                \n",
    "            with self: \n",
    "                tokens = super().parse(pidgy.util.whiten(text), default_rules)\n",
    "            if not self.depth: \n",
    "                if normalize:\n",
    "                    tokens = self.normalizer(text, tokens)\n",
    "                if front_matter: tokens.insert(0, front_matter)\n",
    "            return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the tokens\n",
    "\n",
    "Tokenizing [Markdown] typically extracts conventions at both the block and inline level.\n",
    "Fortunately, `pidgy`'s translation is restricted to block level [Markdown] tokens, and mitigating some potential complexities from having opinions about inline code while tangling.\n",
    "\n",
    "<details><summary><code>normalizer</code></summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class Normalizer(pidgy.util.ContextDepth):\n",
    "        def normalize_code(self, body):\n",
    "            self.compacted.append({'type': 'code', 'lang': None, 'text': body, 'depth': self.depth})\n",
    "        \n",
    "        def normalize_paragraph(self, text, type='paragraph'):\n",
    "            if self.compacted and self.compacted[-1]['type'] == type and self.compacted[-1]['depth'] == self.depth:\n",
    "                self.compacted[-1]['text'] += text\n",
    "            else:\n",
    "                self.compacted.append({'type': type, 'text': text, 'depth': self.depth})\n",
    "\n",
    "        def split_text_on_token(self, text, token):\n",
    "            block, body = token.get('text', '').splitlines(), \"\"\n",
    "            while block:\n",
    "                line = block.pop(0)\n",
    "                if line:\n",
    "                    before, line, text = text.partition(line)\n",
    "                    body += before + line\n",
    "            return body, text\n",
    "        \n",
    "        def normalize_list(self, text, token, tokens):\n",
    "            self.compacted.append({'type': 'list', 'text': \"\", 'depth': self.depth, 'ordered': token['ordered']})\n",
    "            return self(text, tokens, 'list')\n",
    "        \n",
    "        def __call__(self, text, tokens, default_type='paragraph'):\n",
    "            if not self.depth: self.compacted = []\n",
    "            while tokens:\n",
    "                token = tokens.pop(0)\n",
    "                body, text = self.split_text_on_token(text, token)\n",
    "                if token['type']=='code':\n",
    "                    self.normalize_code(body)\n",
    "                elif token['type'] == 'list_start':\n",
    "                    with self:\n",
    "                        text = self.normalize_list(text, token, tokens)\n",
    "                elif token['type'] == 'list_end':\n",
    "                    return text\n",
    "                else:\n",
    "                    self.normalize_paragraph(body, default_type)\n",
    "            self.normalize_paragraph(text, default_type)\n",
    "\n",
    "            return [x for x in self.compacted if x.get('text', 'pass')]\n",
    "        \n",
    "    BlockLexer.normalizer = Normalizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening the tokens to a [Python] string.\n",
    "\n",
    "The tokenizer controls the translation of markdown strings to python strings.  Our major constraint is that the Markdown input should retain line numbers.\n",
    "\n",
    "<details><summary><code>Flatten</code></summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class Tangle(BlockLexer):\n",
    "        def resolve_code(self, object, INDENT):\n",
    "            indent = INDENT\n",
    "            if object.lstrip().startswith(pidgy.util.FENCE):\n",
    "                object = ''.join(''.join(object.partition(pidgy.util.FENCE)[::2]).rpartition(pidgy.util.FENCE)[::2])\n",
    "                indent = INDENT + pidgy.util.num_first_indent(object)\n",
    "                object = textwrap.indent(object, INDENT*pidgy.util.SPACE)\n",
    "            if object.lstrip().startswith(pidgy.util.MAGIC):  ...\n",
    "            else: indent = pidgy.util.num_last_indent(object)\n",
    "            return object, indent\n",
    "\n",
    "        def resolve_list(self, object, indent, source):\n",
    "            lines = object.splitlines(True)\n",
    "            while not lines[0].strip():\n",
    "                line = lines.pop(0)\n",
    "                if not line.strip(): source += line\n",
    "\n",
    "            object = pidgy.util.quote(''.join(lines))\n",
    "            object = rF\"\"\"__import__('re').split(r\"\\n*{mistune.BlockGrammar.list_bullet.pattern[1:]}\", {object})[1:]\"\"\"\n",
    "            object = textwrap.indent(object, indent*pidgy.util.SPACE)\n",
    "            return object, source\n",
    "        \n",
    "        def resolve_front_matter(self, object, indent, token): return textwrap.indent(\n",
    "            F\"locals().update(__import__('ruamel.yaml').yaml.safe_load({pidgy.util.quote(object)}))\\n\"\n",
    "            if token['lang'] == 'yaml' else \n",
    "            F\"locals().update(__import__('toml').loads({pidgy.util.quote(object)}))\\n\", \n",
    "            indent*pidgy.util.SPACE)\n",
    "        \n",
    "        def resolve_block_string(self, object, indent, tokens, source):\n",
    "            object = textwrap.indent(\n",
    "                object, pidgy.util.SPACE*max(\n",
    "                    indent-pidgy.util.num_first_indent(object), 0))\n",
    "            # peek ahead a code objects\n",
    "            for next in tokens:\n",
    "                if next['type'] == 'code':\n",
    "                    next = pidgy.util.num_first_indent(next['text'])\n",
    "                    break\n",
    "            else: next = indent       \n",
    "            Δ = max(next-indent, 0)\n",
    "\n",
    "            if not Δ and source.endswith(pidgy.util.COLON): \n",
    "                Δ += 4\n",
    "\n",
    "            spaces = pidgy.util.indents(object)\n",
    "            object = object[:spaces] + Δ*pidgy.util.SPACE+ object[spaces:]\n",
    "            if not source.rstrip().endswith(pidgy.util.QUOTES): \n",
    "                object = pidgy.util.quote(object)\n",
    "            return object\n",
    "\n",
    "        def stringify(self, tokens: typing.List[dict], source: str = \"\"\"\"\"\", last: int =0) -> str:\n",
    "            import textwrap\n",
    "            INDENT = indent = pidgy.util.base_indent(tokens) or 4\n",
    "            for i, token in enumerate(tokens):\n",
    "                object = token['text']\n",
    "                if token and token['type'] == 'code':\n",
    "                    \"\"\"Convert code tokens.\"\"\"\n",
    "                    object, indent = self.resolve_code(object, INDENT)\n",
    "                elif token and token['type'] == 'front_matter': \n",
    "                    \"\"\"Convert front otkens.\"\"\"\n",
    "                    object = self.resolve_front_matter(object, indent, token)\n",
    "                \n",
    "                elif token and token['type'] == 'list':\n",
    "                    object, source = self.resolve_list(object, indent, source)\n",
    "                elif not object: ...\n",
    "                else:\n",
    "                    object = self.resolve_block_string(object, indent, tokens[i:], source)\n",
    "                \n",
    "                source, object = self.resolve_continuation(source, object, indent)\n",
    "                source += object\n",
    "\n",
    "            # add a semicolon to the source if the last block is code.\n",
    "            for token in reversed(tokens):\n",
    "                if token['text'].strip():\n",
    "                    if token['type'] != 'code': \n",
    "                        source = source.rstrip() + pidgy.util.SEMI\n",
    "                    break\n",
    "\n",
    "            return source\n",
    "        \n",
    "        def resolve_continuation(self, source, object, indent):\n",
    "            if source.rstrip().endswith(pidgy.util.CONTINUATION):\n",
    "                if not source.endswith('\\n'):\n",
    "                    source += \"\\n\"\n",
    "                NEW_CONTINUATION = pidgy.util.SPACE*indent + '\\\\\\n'\n",
    "                lines = source.splitlines(True)\n",
    "                for ct, line in enumerate(reversed(lines)):\n",
    "                    if not line.strip(): continue\n",
    "                    break\n",
    "                if ct:\n",
    "                    lines[-ct:] = [NEW_CONTINUATION]*ct\n",
    "                source = ''.join(lines)\n",
    "                lines = object.splitlines(True)\n",
    "                for ct, line in enumerate(lines):\n",
    "                    if not line.strip(): continue\n",
    "                    break\n",
    "                if ct:\n",
    "                    lines[:ct] = [NEW_CONTINUATION] * ct\n",
    "                object = ''.join(lines)\n",
    "            return source, object\n",
    "        \n",
    "        def __call__(self, str):\n",
    "            return self.stringify(self.parse(str))\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append the lexer for nested rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "        class Tangle(markdown_it.MarkdownIt):\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                super().__init__(*args, **kwargs)\n",
    "                [self.block.ruler.before(\n",
    "                    \"code\",\n",
    "                    \"front_matter\",\n",
    "                    __import__('functools').partial(pidgy.util.frontMatter, x),\n",
    "                    {\"alt\": [\"paragraph\", \"reference\", \"blockquote\", \"list\"]},\n",
    "                ) for x in \"-+\"]\n",
    "                self.block.ruler.before(\n",
    "                    \"reference\", \"footnote_def\", markdown_it.extensions.footnote.index.footnote_def, {\"alt\": [\"paragraph\", \"reference\"]}\n",
    "                )\n",
    "                self.disable('html_block')\n",
    "            def parse(self, src, env=None):\n",
    "                src = pidgy.util.enforce_blanklines(src)\n",
    "                if env is None:\n",
    "                    env = markdown_it.utils.AttrDict()\n",
    "                env.update(src=src.splitlines(True))\n",
    "                return super().parse(src, env)\n",
    "            def render(self, src, env=None):                \n",
    "                env  = markdown_it.utils.AttrDict()\n",
    "                return super().render(src, env)\n",
    "            def stringify(self, src, env=None):               \n",
    "                env = env or markdown_it.utils.AttrDict()\n",
    "                return self.render(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for x in \"default_rules footnote_rules list_rules\".split():\n",
    "        setattr(BlockLexer, x, list(getattr(BlockLexer, x)))\n",
    "        getattr(BlockLexer, x).insert(getattr(BlockLexer, x).index('block_code'), 'doctest')\n",
    "        if 'block_html' in getattr(BlockLexer, x):\n",
    "            getattr(BlockLexer, x).pop(getattr(BlockLexer, x).index('block_html'))\n",
    "    del x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More `pidgy` langauge features\n",
    "\n",
    "`pidgy` experiments extra language features for python, using the same system\n",
    "that IPython uses to add features like line and cell magics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, IPython introduced a convention that allows top level await statements outside of functions. Building of this convenience, `pidgy` allows for top-level __return__ and __yield__ statements.  These statements are replaced with the an IPython display statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class ExtraSyntax(ast.NodeTransformer):\n",
    "        def visit_FunctionDef(self, node): return node\n",
    "        visit_AsyncFunctionDef = visit_FunctionDef        \n",
    "\n",
    "        def visit_Return(self, node):\n",
    "            replace = ast.parse('''__import__('IPython').display.display()''').body[0]\n",
    "            replace.value.args = node.value.elts if isinstance(node.value, ast.Tuple) else [node.value]\n",
    "            return ast.copy_location(replace, node)\n",
    "\n",
    "        def visit_Expr(self, node):\n",
    "            if isinstance(node.value, (ast.Yield, ast.YieldFrom)):  return ast.copy_location(self.visit_Return(node.value), node)\n",
    "            return node\n",
    "\n",
    "        visit_Expression = visit_Expr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know naming is hard, there is no point focusing on it. `pidgy` allows authors\n",
    "to use emojis as variables in python. They add extra color and expression to the narrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def demojize(lines, delimiters=('_', '_')):\n",
    "        str = ''.join(lines or [])\n",
    "        import tokenize, emoji, stringcase; tokens = []\n",
    "        try:\n",
    "            for token in list(tokenize.tokenize(\n",
    "                __import__('io').BytesIO(str.encode()).readline)):\n",
    "                if token.type == tokenize.ERRORTOKEN:\n",
    "                    string = emoji.demojize(token.string, delimiters=delimiters\n",
    "                                           ).replace('-', '_').replace(\"’\", \"_\")\n",
    "                    if tokens and tokens[-1].type == tokenize.NAME: tokens[-1] = tokenize.TokenInfo(tokens[-1].type, tokens[-1].string + string, tokens[-1].start, tokens[-1].end, tokens[-1].line)\n",
    "                    else: tokens.append(\n",
    "                        tokenize.TokenInfo(\n",
    "                            tokenize.NAME, string, token.start, token.end, token.line))\n",
    "                else: tokens.append(token)\n",
    "            return tokenize.untokenize(tokens).decode()\n",
    "        except BaseException: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def init_json():\n",
    "        import builtins\n",
    "        builtins.yes = builtins.true = True\n",
    "        builtins.no = builtins.false = False\n",
    "        builtins.null = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    from poser import *\n",
    "\n",
    "    λ('shell.md').Path().read_text()[print]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    md = Tangle(renderer_cls=PythonRender)\n",
    "\n",
    "    from poser import *\n",
    "\n",
    "    λ('kernel.md').Path().read_text()[md.render][print]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    md = Tangle(renderer_cls=PythonRender)\n",
    "\n",
    "    from poser import *\n",
    "\n",
    "    λ('kernel.md').Path().read_text()[md.render'][print]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    md = Tangle(renderer_cls=PythonRender)\n",
    "\n",
    "    from poser import *\n",
    "\n",
    "    λ[md.render][print](\"\"\"\n",
    "            def f():\n",
    "    this does it\n",
    "    \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
